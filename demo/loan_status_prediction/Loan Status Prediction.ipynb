{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loan Status Prediction\n",
    "\n",
    "Credit Risk Models are one of the most successful applications of data science. The problem can be put as the prediction of how much financially-reliable a person is, given a set of information(data).\n",
    "The implementation of such predictive models has raised many discussions concerning privacy, racial profiling etc.\n",
    "\n",
    "We will be using an anonymized dataset that consists of a set of attributes for a bank's users.\n",
    "You may find the dataset [here](https://www.kaggle.com/zaurbegiev/my-dataset).\n",
    "\n",
    "Let us first install sklearn and numpy libraries:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install scikit-learn numpy matplotlib pandas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's import all the required libraries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from sklearn import preprocessing\n",
    "from sklearn.impute import SimpleImputer as Imputer\n",
    "from sklearn.model_selection import train_test_split, cross_val_score\n",
    "from sklearn.metrics import mean_squared_error, accuracy_score\n",
    "\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "from warnings import filterwarnings\n",
    "filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Raw Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"credit_train.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df[:10000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Feature Selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cols_to_remove = ['Loan ID','Customer ID']\n",
    "data = df.drop(cols_to_remove, axis=1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Data Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Imputation strategy: Replace Non-existing values with the respective column's average\n",
    "cols_to_clean =['Current Loan Amount','Credit Score','Annual Income','Years of Credit History',\n",
    "        'Months since last delinquent','Number of Open Accounts','Number of Credit Problems',\n",
    "       'Current Credit Balance','Maximum Open Credit','Bankruptcies','Tax Liens']\n",
    "\n",
    "imputer = Imputer()\n",
    "data[cols_to_clean] = imputer.fit_transform(data[cols_to_clean])\n",
    "data[cols_to_clean] = data[cols_to_clean].astype(int)\n",
    "\n",
    "#Remove rows that still contain one or more NaN values\n",
    "data=data.dropna()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Feature Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Convert our target attribute to numerical values\n",
    "y = []\n",
    "for i in data['Loan Status']:\n",
    "    if i == 'Fully Paid':\n",
    "        y.append(1)\n",
    "    else:\n",
    "        y.append(0)\n",
    "\n",
    "data = data.drop('Loan Status', axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert categorical attributes to numerical values\n",
    "print(data.info())\n",
    "data = pd.get_dummies(data)\n",
    "print(data.info())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data Normalization -- Version one\n",
    "# All variables will have a mean zero and variance/standard deviation of 1\n",
    "xMean = np.mean(data, axis=0)\n",
    "xDev = np.std(data, axis=0)\n",
    "xNorm = (data - xMean) / xDev"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data Normalization -- Version two\n",
    "# All features are scaled within a given range (0.0-1.0 by default)\n",
    "\n",
    "x = data.values #returns numpy array\n",
    "min_max_scaler = preprocessing.MinMaxScaler()\n",
    "x_scaled = min_max_scaler.fit_transform(x)\n",
    "xMinMax = pd.DataFrame(x_scaled)\n",
    "xNoNorm = data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xNorm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we will see how we get different predictive performances by using the raw features vs the two other feature normalization techniques."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_versions = []\n",
    "# Define three cases we will study\n",
    "for x in [xNorm, xMinMax, xNoNorm]:\n",
    "    version = {}\n",
    "    version['x_train'], version['x_test'], version['y_train'], version['y_test'] = train_test_split(x, y, test_size= 0.25, random_state=13)\n",
    "    data_versions.append(version)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Training and Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the classifiers that we will be testing\n",
    "clf = KNeighborsClassifier(n_neighbors=2)\n",
    "\n",
    "for i, data_version in enumerate(data_versions):\n",
    "    print(\"Evaluating the model with data from the version #{}\".format(i+1))\n",
    "    #train model with train data\n",
    "    clf.fit(X=data_version['x_train'], y=data_version['y_train'])\n",
    "        \n",
    "    #predict test data\n",
    "    predictions = clf.predict(X=data_version['x_test'])\n",
    "        \n",
    "    #calculate the accuracy\n",
    "    accuracy = accuracy_score(data_version['y_test'], predictions)\n",
    "\n",
    "    print(\"\\t Classifier  achieved {} accuracy on test data.\".format(100*accuracy))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TODO: \n",
    "Follow a similar approach with the one described in the previous cell.\n",
    "<br>Evaluate the performance of Logistic Regression and Random Forest classifiers. \n",
    "<br>Try to find a set of parameters that work the best for each classifier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using: Logistic Regression Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using: Random Forest Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Evaluate the performance of the best combination, using the k-fold cross-validation approach"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Implement Cross Validation using Logistic Regression classifier\n",
    "# Using xMinMax dataframe\n",
    "\n",
    "clf = <BEST CLASSIFIER>\n",
    "X_data = <BEST DATA FORMAT>\n",
    "\n",
    "# 5 fold cross validation\n",
    "scores = cross_val_score(clf, X_data, y, verbose=1, cv=5)\n",
    "print(scores)\n",
    "print(\"Accuracy: %0.3f (+/- %0.3f)\" % (scores.mean(), scores.std() * 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### TODO: Evaluate the performance of "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
